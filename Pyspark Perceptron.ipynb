{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eccda4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f94f4b82cb0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler, Normalizer\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\"NN music class\").getOrCreate()\n",
    "\n",
    "\n",
    "# import csv files\n",
    "df1 = spark.read.csv(\n",
    "    path=\"/user1/songs.csv\",\n",
    "    sep='\\t',\n",
    "    header = True,\n",
    "    quote = '\"',\n",
    "    inferSchema= True)\n",
    "\n",
    "df2 = spark.read.csv(\n",
    "    path=\"/user1/acoustic_features.csv\",\n",
    "    sep='\\t',\n",
    "    header = True,\n",
    "    quote = '\"',\n",
    "    inferSchema= True)\n",
    "\n",
    "\n",
    "# Create views for df\n",
    "df1.createOrReplaceTempView(\"features\")\n",
    "df2.createOrReplaceTempView(\"songs\")\n",
    "\n",
    "#show spark UI\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08e80dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+---------+-------+----------+--------+---------+\n",
      "|song_id|song_name|billboard|artists|popularity|explicit|song_type|\n",
      "+-------+---------+---------+-------+----------+--------+---------+\n",
      "|  3e...|    th...|    ('...|  {'...|        86|    true|     Solo|\n",
      "|  5p...|    Wi...|    ('...|  {'...|        87|    true|     Solo|\n",
      "|  2x...|    SI...|    ('...|  {'...|        85|    true|     Solo|\n",
      "|  3K...|    Su...|    ('...|  {'...|        92|   false|    Co...|\n",
      "|  1r...|    Hi...|    ('...|  {'...|        86|   false|     Solo|\n",
      "|  0b...|    Al...|    ('...|  {'...|        63|   false|     Solo|\n",
      "|  5h...|    It...|    (\"...|  {'...|        52|   false|     Solo|\n",
      "|  2E...|    Ro...|    (\"...|  {'...|        53|   false|     Solo|\n",
      "|  0M...|    A ...|    ('...|  {'...|         3|   false|     Solo|\n",
      "|  6Z...|    Ji...|    ('...|  {'...|        51|   false|     Solo|\n",
      "|  1x...|    Mo...|    ('...|  {'...|        81|    true|     Solo|\n",
      "|  4P...|    Th...|    ('...|  {'...|        48|   false|     Solo|\n",
      "|  4O...|    br...|    ('...|  {'...|        78|    true|     Solo|\n",
      "|  55...|    Yo...|    ('...|  {'...|        77|   false|     Solo|\n",
      "|  7d...|    Be...|    ('...|  {'...|        86|    true|     Solo|\n",
      "|  28...|    Lu...|    ('...|  {'...|        85|    true|     Solo|\n",
      "|  39...|    im...|    ('...|  {'...|        76|    true|     Solo|\n",
      "|  2F...|    La...|    ('...|  {'...|        55|   false|     Solo|\n",
      "|  1d...|    Ru...|    ('...|  {'...|        36|   false|     Solo|\n",
      "|  6C...|     Trip|    ('...|  {'...|        77|   false|     Solo|\n",
      "+-------+---------+---------+-------+----------+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show(truncate=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08e5b3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL join df1+df2\n",
    "joined_df = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM features\n",
    "    INNER JOIN songs ON features.song_id = songs.song_id\n",
    "\"\"\")\n",
    "\n",
    "#drop columns\n",
    "for x in ['songs.song_id','song_name','billboard','artists','features.song_id']:\n",
    "    joined_df = joined_df.drop(joined_df[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e91e44e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|time_signature|\n",
      "+--------------+\n",
      "|             1|\n",
      "|             3|\n",
      "|             5|\n",
      "|             4|\n",
      "|             0|\n",
      "+--------------+\n",
      "\n",
      "+---+\n",
      "|key|\n",
      "+---+\n",
      "|  1|\n",
      "|  6|\n",
      "|  3|\n",
      "|  5|\n",
      "|  9|\n",
      "|  4|\n",
      "|  8|\n",
      "|  7|\n",
      "| 10|\n",
      "| 11|\n",
      "|  2|\n",
      "|  0|\n",
      "+---+\n",
      "\n",
      "+----+\n",
      "|mode|\n",
      "+----+\n",
      "|   1|\n",
      "|   0|\n",
      "+----+\n",
      "\n",
      "+-------------+\n",
      "|    song_type|\n",
      "+-------------+\n",
      "|Collaboration|\n",
      "|         Solo|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#SQL queries before feature engineering\n",
    "joined_df.createOrReplaceTempView(\"df\")\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT DISTINCT time_signature\n",
    "    FROM df\n",
    "\"\"\")\n",
    "result.show()\n",
    "\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT DISTINCT key\n",
    "    FROM df\n",
    "\"\"\")\n",
    "result.show()\n",
    "\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT DISTINCT mode\n",
    "\n",
    "    FROM df\n",
    "\"\"\")\n",
    "result.show()\n",
    "\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT DISTINCT song_type\n",
    "\n",
    "    FROM df\n",
    "\"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3efbd02b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- popularity: integer (nullable = false)\n",
      " |-- explicit: boolean (nullable = true)\n",
      " |-- song_type: integer (nullable = false)\n",
      " |-- duration_ms: integer (nullable = true)\n",
      " |-- mode: integer (nullable = true)\n",
      " |-- time_signature: integer (nullable = true)\n",
      " |-- acousticness: double (nullable = true)\n",
      " |-- danceability: double (nullable = true)\n",
      " |-- energy: double (nullable = true)\n",
      " |-- instrumentalness: double (nullable = true)\n",
      " |-- liveness: double (nullable = true)\n",
      " |-- loudness: double (nullable = true)\n",
      " |-- speechiness: double (nullable = true)\n",
      " |-- valence: double (nullable = true)\n",
      " |-- tempo: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#feature engineering based on EDA\n",
    "from pyspark.sql.functions import col,when\n",
    "\n",
    "joined_df = joined_df.withColumn('song_type',when(col('song_type')=='Solo',1).otherwise(0))\n",
    "joined_df = joined_df.withColumn('popularity',when(col('popularity')>=50,1).otherwise(0))\n",
    "joined_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4819866",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-23 01:12:37,851 WARN util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "| normalized_features|\n",
      "+--------------------+\n",
      "|[4.82346067092288...|\n",
      "|[4.95881589199965...|\n",
      "|[3.19672615899143...|\n",
      "|[0.0,0.0,0.999999...|\n",
      "|[0.0,5.2370548235...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Normalize and create vector for NN\n",
    "columns_to_normalize = [\n",
    " 'explicit',\n",
    " 'song_type',\n",
    " 'duration_ms',\n",
    " 'mode',\n",
    " 'time_signature',\n",
    " 'acousticness',\n",
    " 'danceability',\n",
    " 'energy',\n",
    " 'instrumentalness',\n",
    " 'liveness',\n",
    " 'loudness',\n",
    " 'speechiness',\n",
    " 'valence',\n",
    " 'tempo']\n",
    "\n",
    "#assembler\n",
    "assembler = VectorAssembler(inputCols=columns_to_normalize, outputCol=\"features\")\n",
    "\n",
    "#normalizer\n",
    "normalizer = Normalizer(inputCol=\"features\", outputCol=\"normalized_features\", p=2.0)\n",
    "\n",
    "#put all together in pipeline\n",
    "pipeline = Pipeline(stages=[assembler, normalizer])\n",
    "\n",
    "#final df\n",
    "normalized_df = pipeline.fit(joined_df).transform(joined_df)\n",
    "#show normalized vector\n",
    "normalized_df.select(\"normalized_features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2baf7457",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-23 01:12:55,220 WARN netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "2023-09-23 01:12:55,221 WARN netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7087\n",
      "f1: 0.5878\n",
      "weightedPrecision: 0.5022\n",
      "weightedRecall: 0.7087\n"
     ]
    }
   ],
   "source": [
    "#NN creation\n",
    "layers = [14, 64, 32, 16, 2] \n",
    "\n",
    "#Neural network params\n",
    "classifier = MultilayerPerceptronClassifier(\n",
    "    layers=layers,\n",
    "    labelCol=\"popularity\",  \n",
    "    featuresCol=\"normalized_features\",\n",
    "    maxIter=100,            \n",
    "    blockSize=128,         \n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# final pipeline\n",
    "pipeline = Pipeline(stages=[assembler, normalizer, classifier])\n",
    "\n",
    "# Train test split\n",
    "\n",
    "train_ratio = .8\n",
    "test_ratio = .2\n",
    "train_data, test_data = joined_df.randomSplit([train_ratio, test_ratio], seed=42)\n",
    "\n",
    "# Fit the model \n",
    "model = pipeline.fit(train_data)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Evaluation pf different metrics\n",
    "\n",
    "metric_names = [\"accuracy\", \"f1\", \"weightedPrecision\", \"weightedRecall\"]\n",
    "\n",
    "for metric_name in metric_names:\n",
    "\n",
    "    evaluator = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"popularity\",\n",
    "        predictionCol=\"prediction\",\n",
    "        metricName=metric_name)\n",
    "\n",
    "\n",
    "# print each metric\n",
    "    metric_value = evaluator.evaluate(predictions, {evaluator.metricName: metric_name})\n",
    "    print(f\"{metric_name}: {metric_value:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8a8920b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blockSize: block size for stacking input data in matrices. Data is stacked within partitions. If block size is more than remaining data in a partition then it is adjusted to the size of this data. (default: 128, current: 128)\n",
      "featuresCol: features column name. (default: features, current: normalized_features)\n",
      "initialWeights: The initial weights of the model. (undefined)\n",
      "labelCol: label column name. (default: label, current: popularity)\n",
      "layers: Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons. (current: [14, 64, 32, 16, 2])\n",
      "maxIter: max number of iterations (>= 0). (default: 100, current: 100)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n",
      "rawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\n",
      "seed: random seed. (default: -4740013646608911034, current: 42)\n",
      "solver: The solver algorithm for optimization. Supported options: l-bfgs, gd. (default: l-bfgs)\n",
      "stepSize: Step size to be used for each iteration of optimization (>= 0). (default: 0.03)\n",
      "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\n"
     ]
    }
   ],
   "source": [
    "print(classifier.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23149b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c54e5ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
